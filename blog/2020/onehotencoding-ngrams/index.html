<!DOCTYPE html>
<html>

<!-- Mathjax Support -->
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Sumon Kanti Dey | one-hot encoding & n-gram feature"</title>
  <meta name="description"
    content="Engineer Scientist. at  Leadbook">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <!-- <link rel="canonical"
    href="/blog/2017/replication-of-study-2-in-bias-in-the-flesh-skin-complexion-and-stereotype-consistency-in-political-campaigns/">
</head> -->


<body>

  <header class="site-header">

    <div class="wrapper">


      <span class="site-title">

        <strong>Sumon Kanti Dey</strong>
      </span>


      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242"
                d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z" />
              <path fill="#424242"
                d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z" />
              <path fill="#424242"
                d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z" />
            </svg>
          </span>
        </label>
        <div class="trigger">
          <!-- About -->
          <a class="page-link" href="/">About</a>
          <!-- Blog -->
          <a class="page-link" href="/blog/">Posts</a>
          <!-- Pages -->
          <a class="page-link" href="/projects/">Projects</a>
          <!-- CV -->
          <a class="page-link" href="/assets/pdf/Sumon_Kanti_Dey_Resume.pdf">Resume</a>
          <!-- <a class="page-link" href="/publications/">publications</a>
        <a class="page-link" href="/teaching/">teaching</a> -->
        </div>
      </nav>

    </div>

  </header>



  <div class="page-content">
    <div class="wrapper">
      <div class="post">

        <header class="post-header">
          <h1 class="post-title">shortcomings of one-hot encoding & n-gram feature in text classification</h1>
          <p class="post-meta">May 30, 2020</p>
        </header>

        <article class="post-content">
          <!-- <p>
            Word Embedding is a vector representation of word which is used for measuring semantic and syntactic
            similarity, relation with other words in a document.
          </p> -->
          <!-- <p>
            In this 
            There are many options to represent a word from a document, suppose one-hot encoding and n-grams.
            But those methods can not make word to word relation means can not create word similarity. Let's deep dive.

          </p> -->
          <p>
            In this tutorial, I have tried to describe the basic limitation of one-hot encoding and n-grams feature for
            finding text similarity.
          </p>
          <h4>one hot encoding</h4>
          <p>
            For text encoding, there are few techniques available one-hot encoding is one of them.
            one hot encoding helps to map a word in one-hot vector.
            Let's think about two similar sentences <i>" she was a beautiful woman"</i> and
            <i>"she was a wonderful woman"</i>.
            If we construct a vocabulary (let's call it V) from those sentences,
            it would have V = {she, was, a, beautiful, wonderful, woman}.
            Now we can create a one-hot vector each of the words in V.
            The length of the one-hot encoded vector would be equal to the size of (V=6).
            So for one-hot encoding,
            we will consider a vector of zeros except for the representation of the vocabulary word index which will be
            considered as one.
            The encoding below would be explained better.
          </p>

          <ol>
            she = [1,0,0,0,0,0,0]
            was = [0,1,0,0,0,0]
            a = [0,0,1,0,0,0]
            beautiful = [0,0,0,1,0,0]
            wonderful = [0,0,0,0,1,0]
            woman = [0,0,0,0,0,1]
          </ol>
          <p>
            In one hot encoding representation, all of the words are independent of each other.
            If we try to visualize this encoding, we can think of a 6-dimensional space, Where one word occupies one of
            the dimensions and can not make any projection along with other dimensions. That means <i>"beautiful"</i>
            and <i>"wonderful"</i> this two words
            considered as different as rest of the other words, which is not true. If Vocabulary size increase it will
            make a huge sparse matrix which will consume more memory and space.
          </p>
          <h4>N-grams</h4>

          <p>
            n-grams can be used to represent the relation between words in a document. n-grams help to measure the
            number of co-occurring words within a window. For example, for the sentence <i>"she is a wonderful
              woman"</i> (if N=2 known as bigrams) then the n-grams would be
          </p>
          <ul style="list-style-type:disc">
            <li>she is</li>
            <li>is a</li>
            <li>a wonderful</li>
            <li>wonderful woman</li>
          </ul>
          <p>
            we have 4 n-grams in this case.<br>
            If N = 3 known as trigrams then n-grams would be:
          </p>
          <ul>
            <li>she is a</li>
            <li>is a wonderful</li>
            <li>a wonderful woman</li>
          </ul>
          <p>
            So we have 3 n-grams in this case. <br>
            n-grams of given sentence K would be <i>X - (N-1)</i>
            Where, <i>X = number of words in given sentence K</i>
          </p>
          <p><img src="/assets/img/ngrams.png" alt="" /></p>
          <p>The major drawback of the n-grams feature is increasing data sparsity,
            which means more data needed to successfully train a statistical model. Also,
            n-grams can not resolve the input feature space, which increases exponentially with the n-grams feature.
          </p>
        </article>






      </div>

    </div>
  </div>

  <footer>

    <div class="wrapper">
      Â© Copyright 2020 Sumon Kanti Dey. Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with
      <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a
          href="https://pages.github.com/" target="_blank">GitHub Pages</a>.
    </div>
  </footer>


  <!-- Load jQuery -->
  <script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>


  <!-- Load KaTeX -->
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script src="/assets/js/katex.js"></script>




  <!-- Include custom icon fonts -->
  <link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">

  <!-- Google Analytics -->
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-27273977-1', 'auto');
    ga('send', 'pageview');
  </script>


</body>

</html>